from dataclasses import dataclass
from typing import Dict, Tuple

@dataclass
class ModelConfig:
    name: str
    num_layers: int
    hidden_size: int
    intermediate_size: int
    num_heads: int = 32
    vocab_size: int = 50257
    seq_len: int = 1024

@dataclass
class HardwareConfig:
    name: str
    peak_flops: float             # in FLOPs/sec
    memory_bandwidth: float       # in bytes/sec
    storage_bandwidth: float      # bytes/sec for model load
    host_to_device_bw: float      # bytes/sec for PCIe
    network_bandwidth: float      # bytes/sec for distributed shards
    compute_util: float = 0.7     # effective utilization of peak FLOPs
    memory_util: float = 0.6      # effective utilization of memory BW
    storage_util: float = 0.8     # effective util for storage BW
    h2d_util: float = 0.7         # effective util for hostâ†’device BW
    net_util: float = 0.8         # effective util for network BW
    energy_per_flop: float = 1e-12    # Joules per FLOP
    energy_per_byte: float = 1e-9     # Joules per accessed byte

@dataclass
class PrecisionConfig:
    name: str
    dtype_size: int               # bytes per element

class EdgeProfile:
    def __init__(self, model: ModelConfig, hw: HardwareConfig, prec: PrecisionConfig):
        self.model = model
        self.hw = hw
        self.prec = prec

    def parameter_count(self) -> int:
        # Include weights for projections and FFN and embeddings
        L, H, I, V = self.model.num_layers, self.model.hidden_size, self.model.intermediate_size, self.model.vocab_size
        
        # Attention projections: Q,K,V,O per layer
        proj = L * 4 * H * H
        
        # FFN: up_proj (H->I), down_proj (I->H), gate_proj (H->I) for modern LLMs
        ffn = L * (H * I + I * H + H * I)  # 3 projections instead of 2
        
        # Layer normalization: input_norm + post_attention_norm per layer
        layer_norms = L * 2 * H
        
        # Embeddings: token embeddings + output head (often shared)
        embed = V * H + H * V  # Separate input and output embeddings
        
        # RMSNorm final layer
        final_norm = H
        
        total = proj + ffn + layer_norms + embed + final_norm
        return total

    def flops_per_token(self) -> float:
        L, H, I, S = self.model.num_layers, self.model.hidden_size, self.model.intermediate_size, self.model.seq_len
        attn_proj = L * (3 * 2 * H * H + 2 * H * H)
        attn_kv   = L * (2 * H * S + 2 * H * S)
        ffn       = L * (2 * H * I + 2 * I * H)
        ln        = L * (2 * H + 2 * H)
        softmax   = L * (5 * H)
        return attn_proj + attn_kv + ffn + ln + softmax

    def memory_footprint(self) -> float:
        # Weights + activations + KV cache
        params = self.parameter_count()
        weight_bytes = params * self.prec.dtype_size
         # Activations: hidden states for full sequence
        act_bytes    = self.model.seq_len * self.model.hidden_size * self.prec.dtype_size
        # KV cache: stored keys & values per layer
        kv_bytes     = self.model.num_layers * self.model.seq_len * self.model.hidden_size * 2 * self.prec.dtype_size
        return weight_bytes + act_bytes + kv_bytes

    def breakdown_times(self) -> Dict[str, float]:
        L, H, I, S = self.model.num_layers, self.model.hidden_size, self.model.intermediate_size, self.model.seq_len
        # raw FLOPs
        attn_proj = L * (3 * 2 * H * H + 2 * H * H)
        attn_kv   = L * (2 * H * S + 2 * H * S)
        ffn       = L * (2 * H * I + 2 * I * H)
        ln        = L * (2 * H + 2 * H)
        softmax   = L * (5 * H)
        # times in seconds
        comp_util = self.hw.peak_flops * self.hw.compute_util
        return {
            'attn_proj_ms': (attn_proj / comp_util) * 1e3,
            'attn_kv_ms':   (attn_kv   / comp_util) * 1e3,
            'ffn_ms':       (ffn       / comp_util) * 1e3,
            'ln_ms':        (ln        / comp_util) * 1e3,
            'softmax_ms':   (softmax   / comp_util) * 1e3,
        }

    def estimate_latency(self) -> Tuple[float, float, float]:
        flops = self.flops_per_token()
        weight_bytes = self.parameter_count() * self.prec.dtype_size
        comp_time = flops / (self.hw.peak_flops * self.hw.compute_util)
        mem_time  = self.memory_footprint() / (self.hw.memory_bandwidth * self.hw.memory_util)
        return comp_time, mem_time, max(comp_time, mem_time)

    def io_time(self, model_loaded=True) -> float:
        # Model loading time - usually only happens once at startup
        if model_loaded:
            return 0.0  # Model already in memory for inference
        weight_bytes = self.parameter_count() * self.prec.dtype_size
        return weight_bytes / (self.hw.storage_bandwidth * self.hw.storage_util)

    def h2d_transfer_time(self, cpu_only=True) -> float:
        # Host-to-Device transfer - only relevant for GPU inference
        if cpu_only:
            return 0.0  # No transfer needed for CPU-only inference
        weight_bytes = self.parameter_count() * self.prec.dtype_size
        return weight_bytes / (self.hw.host_to_device_bw * self.hw.h2d_util)

    def network_transfer_time(self) -> float:
        
        shard_bytes = self.model.seq_len * self.model.hidden_size * self.prec.dtype_size
        return shard_bytes / (self.hw.network_bandwidth * self.hw.net_util)

    def arithmetic_intensity(self) -> float:
        return self.flops_per_token() / self.memory_footprint()

    def energy_estimate(self) -> float:
        e_flop = self.flops_per_token() * self.hw.energy_per_flop
        e_mem  = self.memory_footprint() * self.hw.energy_per_byte
        return e_flop + e_mem

    def full_profile(self, model_loaded=True, cpu_only=True) -> dict:
        comp, mem, base = self.estimate_latency()
        io    = self.io_time(model_loaded=model_loaded)
        h2d   = self.h2d_transfer_time(cpu_only=cpu_only)
        net   = self.network_transfer_time()
        end2end = base + io + h2d + net
        br = self.breakdown_times()

        return {
            'device': self.hw.name,
            'model': self.model.name,
            'precision': self.prec.name,
            'params (M)': self.parameter_count() / 1e6,
            'FLOPs/token (G)': self.flops_per_token() / 1e9,
            'mem_footprint (MB)': self.memory_footprint() / (1024**2),
            'compute_ms': comp  * 1e3,
            'memory_ms':  mem  * 1e3,
            'io_ms':      io   * 1e3,
            'h2d_ms':     h2d  * 1e3,
            'net_ms':     net  * 1e3,
            'end2end_ms': end2end * 1e3,
            'arithmetic_intensity': self.arithmetic_intensity(),
            'energy_J': self.energy_estimate(),
            **br
        }

if __name__ == '__main__':
    # Precisions
    precisions = [
        PrecisionConfig('FP32', 4),
        PrecisionConfig('FP16', 2),
        PrecisionConfig('BF16', 2),  # Better for CPU inference
        PrecisionConfig('INT8', 1),
    ]

    # Models
    models = [
        ModelConfig('TinyLlama-1B',    24,2048,8192, num_heads=32, seq_len=1024),
        ModelConfig('Gemma3-1B',       24,2048,8192, num_heads=32, seq_len=1024),
        ModelConfig('Llama3.2-1B',     24,2048,8192, num_heads=32, seq_len=1024),
        ModelConfig('DeepSeek-r1-1.5B',26,2304,9216, num_heads=36, seq_len=1024),
        # Custom model configuration - CPU-optimized 7B model (updated with actual specs)
        ModelConfig('finetunecyberexpert-INT8', 32, 4096, 11008, num_heads=32, vocab_size=32000, seq_len=2048),
        # ModelConfig('2B', 28, 2560, 10240, num_heads=40, seq_len=1024),
        # ModelConfig('2.5B', 30, 2816,11264, num_heads=44, seq_len=1024),
        # ModelConfig('3B', 32, 3072,12288, num_heads=48, seq_len=1024),
    ]
    # Hardware platforms
    hardware_list = [
        HardwareConfig('Raspberry Pi 4',   50e9, 19e9, 5e8, 12e9, 1e8),
        HardwareConfig('Raspberry Pi 5',   64e9, 34e9, 5e8, 16e9, 1e8),
        HardwareConfig('Jetson Nano Super',0.5e12,25.6e9, 2e9, 30e9, 5e8),
        #isolated system configuration****
        HardwareConfig(
            name='My Isolated System',
            peak_flops=4.416e11,
            memory_bandwidth=8.96e10,
            storage_bandwidth=1.02e8,  # 102 MB/s converted to bytes/s
            host_to_device_bw=8.96e10,  # Using memory bandwidth as specified
            network_bandwidth=1e8,
            compute_util=0.7,
            memory_util=0.6,
            storage_util=0.8,
            h2d_util=0.7,
            net_util=0.8,
            energy_per_flop=2e-12,    # Higher for CPU vs GPU
            energy_per_byte=2e-9      # CPU memory access energy
        ),
    ]
    # Header
    header = (
        f"{'Device':>17} | {'Model':>15} | {'Prec':>5} | {'Params(M)':>9} |"
        f" {'FLOPs/tok(G)':>12} | {'Mem(MB)':>8} | {'Comp(ms)':>8} | {'Mem(ms)':>8} |"
        f" {'IO(ms)':>7} | {'H2D(ms)':>7} | {'Net(ms)':>7} | {'E2E(ms)':>8} |"
        f" {'AI':>6} | {'Energy(J)':>10}"
    )
    print(header)
    print('-' * len(header))

    for hw in hardware_list:
        for model in models:
            for prec in precisions:
                # For CPU inference, model is loaded once and stays in memory
                prof = EdgeProfile(model, hw, prec).full_profile(model_loaded=True, cpu_only=True)
                print(
                    f"{prof['device']:>17} | {prof['model']:>15} | {prof['precision']:>5} | "
                    f"{prof['params (M)']:9.1f} | {prof['FLOPs/token (G)']:12.1f} | "
                    f"{prof['mem_footprint (MB)']:8.1f} | {prof['compute_ms']:8.2f} | "
                    f"{prof['memory_ms']:8.2f} | {prof['io_ms']:7.2f} | {prof['h2d_ms']:7.2f} | "
                    f"{prof['net_ms']:7.2f} | {prof['end2end_ms']:8.2f} | "
                    f"{prof['arithmetic_intensity']:6.2f} | {prof['energy_J']:10.3f}"
                )
